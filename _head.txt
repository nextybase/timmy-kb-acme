# SPDX-License-Identifier: GPL-3.0-or-later
# src/semantic/api.py
from __future__ import annotations

import inspect
import logging
import re
from functools import lru_cache
from pathlib import Path
from typing import TYPE_CHECKING, Any, Dict, List, Optional, Set, Tuple, cast

from kb_db import insert_chunks as _insert_chunks
from pipeline.constants import OUTPUT_DIR_NAME, REPO_NAME_PREFIX
from pipeline.content_utils import convert_files_to_structured_markdown as _convert_md
from pipeline.content_utils import generate_readme_markdown as _gen_readme
from pipeline.content_utils import generate_summary_markdown as _gen_summary
from pipeline.content_utils import validate_markdown_dir as _validate_md
from pipeline.embedding_utils import normalize_embeddings
from pipeline.exceptions import ConfigError, ConversionError
from pipeline.file_utils import safe_write_text
from pipeline.logging_utils import phase_scope
from pipeline.path_utils import ensure_within, sorted_paths
from semantic.auto_tagger import extract_semantic_candidates as _extract_candidates
from semantic.auto_tagger import render_tags_csv as _render_tags_csv
from semantic.config import load_semantic_config as _load_semantic_config
from semantic.normalizer import normalize_tags as _normalize_tags
from semantic.tags_extractor import copy_local_pdfs_to_raw as _copy_local_pdfs_to_raw
from semantic.tags_io import write_tagging_readme as _write_tagging_readme
from semantic.types import EmbeddingsClient as _EmbeddingsClient
from semantic.vocab_loader import load_reviewed_vocab as _load_reviewed_vocab

if TYPE_CHECKING:
    from pipeline.context import ClientContext as ClientContextType
else:
    from semantic.types import ClientContextProtocol as ClientContextType

__all__ = [
    "get_paths",
    "load_reviewed_vocab",
    "convert_markdown",
    "enrich_frontmatter",
    "write_summary_and_readme",
    "build_mapping_from_vision",
    "build_tags_csv",
    "build_markdown_book",
    "index_markdown_to_db",
    "copy_local_pdfs_to_raw",
]


def get_paths(slug: str) -> Dict[str, Path]:
    base_dir = Path(OUTPUT_DIR_NAME) / f"{REPO_NAME_PREFIX}{slug}"
    return {
        "base": base_dir,
        "raw": base_dir / "raw",
        "book": base_dir / "book",
        "semantic": base_dir / "semantic",
    }


def load_reviewed_vocab(base_dir: Path, logger: logging.Logger) -> Dict[str, Dict[str, Set[str]]]:
    return cast(Dict[str, Dict[str, Set[str]]], _load_reviewed_vocab(base_dir, logger))


class _CtxShim:
    base_dir: Path
    raw_dir: Path
    md_dir: Path
    slug: str

    def __init__(self, *, base_dir: Path, raw_dir: Path, md_dir: Path, slug: str) -> None:
        self.base_dir = base_dir
        self.raw_dir = raw_dir
        self.md_dir = md_dir
        self.slug = slug


def _resolve_ctx_paths(context: ClientContextType, slug: str) -> tuple[Path, Path, Path]:
    paths = get_paths(slug)
    base_dir = cast(Path, getattr(context, "base_dir", None) or paths["base"])
    raw_dir = cast(Path, getattr(context, "raw_dir", None) or (base_dir / "raw"))
    md_dir = cast(Path, getattr(context, "md_dir", None) or (base_dir / "book"))
    return base_dir, raw_dir, md_dir


def _call_convert_md(func: Any, ctx: _CtxShim, md_dir: Path) -> None:
    if not callable(func):
        raise ConversionError("convert_md target is not callable", slug=ctx.slug, file_path=md_dir)
    sig = inspect.signature(func)
    params = sig.parameters
    kwargs: Dict[str, Any] = {}
    if "md_dir" in params:
        kwargs["md_dir"] = md_dir
    try:
        bound = sig.bind_partial(ctx, **kwargs)
        bound.apply_defaults()
        func(*bound.args, **bound.kwargs)
    except TypeError as e:
        raise ConversionError(f"convert_md call failed: {e}", slug=ctx.slug, file_path=md_dir) from e


def convert_markdown(context: ClientContextType, logger: logging.Logger, *, slug: str) -> List[Path]:
    """Converte i PDF in RAW in Markdown strutturato dentro book/.

    Regola di idempotenza/robustezza:
    - Se in RAW **non ci sono PDF**, NON invocare il converter (evita cleanup indesiderati)
      e restituisci i Markdown di contenuto già presenti in book/.
    - Se in RAW **ci sono PDF**, invoca sempre il converter.
    """
    base_dir, raw_dir, book_dir = _resolve_ctx_paths(context, slug)
    ensure_within(base_dir, raw_dir)
    ensure_within(base_dir, book_dir)

    if not raw_dir.exists():
        raise ConfigError(f"Cartella RAW locale non trovata: {raw_dir}", slug=slug, file_path=raw_dir)

    book_dir.mkdir(parents=True, exist_ok=True)
    shim = _CtxShim(base_dir=base_dir, raw_dir=raw_dir, md_dir=book_dir, slug=slug)

    # snapshot iniziale dei contenuti
    def _list_content_mds() -> list[Path]:
        return [
            p for p in sorted_paths(book_dir.glob("*.md"), base=book_dir)
            if p.name.lower() not in {"readme.md", "summary.md"}
        ]

    local_pdfs = [p for p in raw_dir.rglob("*") if p.is_file() and p.suffix.lower() == ".pdf"]

    with phase_scope(logger, stage="convert_markdown", customer=slug) as m:
        if local_pdfs:
            _call_convert_md(_convert_md, shim, book_dir)
            content_mds = _list_content_mds()
        else:
            # nessun PDF → non convertiamo; usiamo i contenuti già presenti
            content_mds = _list_content_mds()

        try:
            m.set_artifacts(len(content_mds))
        except Exception:
            m.set_artifacts(None)

    if content_mds:
        return content_mds

    # nessun MD di contenuto: distinguere tra "nessun PDF" e "conversione senza output"
    if not local_pdfs:
        raise ConfigError(f"Nessun PDF trovato in RAW locale: {raw_dir}", slug=slug, file_path=raw_dir)
    raise ConversionError(
        "La conversione non ha prodotto Markdown di contenuto (solo README/SUMMARY).",
        slug=slug,
        file_path=book_dir,
    )


def enrich_frontmatter(
    context: ClientContextType,
    logger: logging.Logger,
    vocab: Dict[str, Dict[str, Set[str]]],
    *,
    slug: str,
) -> List[Path]:
    from pipeline.path_utils import read_text_safe

    base_dir, raw_dir, book_dir = _resolve_ctx_paths(context, slug)  # noqa: F841
    ensure_within(base_dir, book_dir)

    mds = sorted_paths(book_dir.glob("*.md"), base=book_dir)
    touched: List[Path] = []
    inv = _build_inverse_index(vocab)

    with phase_scope(logger, stage="enrich_frontmatter", customer=slug) as m:
        for md in mds:
            name = md.name
            title = re.sub(r"[_\/\-\s]+", " ", Path(name).stem).strip().replace("  ", " ") or "Documento"
            tags = _guess_tags_for_name(name, vocab, inv=inv)
            try:
                text = read_text_safe(book_dir, md, encoding="utf-8")
            except OSError as e:
                logger.warning("Impossibile leggere MD", extra={"file_path": str(md), "error": str(e)})
                continue
            meta, body = _parse_frontmatter(text)
            new_meta = _merge_frontmatter(meta, title=title, tags=tags)
            if meta == new_meta:
                continue
            fm = _dump_frontmatter(new_meta)
            try:
                ensure_within(book_dir, md)
                safe_write_text(md, fm + body, encoding="utf-8", atomic=True)
                touched.append(md)
                logger.info("Frontmatter arricchito", extra={"file_path": str(md), "tags": tags})
            except OSError as e:
                logger.warning("Scrittura MD fallita", extra={"file_path": str(md), "error": str(e)})
        try:
            m.set_artifacts(len(touched))
        except Exception:
            m.set_artifacts(None)
    return touched


def write_summary_and_readme(context: ClientContextType, logger: logging.Logger, *, slug: str) -> None:
